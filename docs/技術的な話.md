# 超大規模言語モデルを分散ネットワーク上で動かす技術
## はじめに
近年、超大規模言語モデル（LLM）は目覚ましい発展を遂げており、様々な分野で応用されています。
しかし、LLMは膨大な計算量とメモリを必要とするため、単一のノードで処理するには限界があります。
そこで、分散ネットワーク上でLLMを実行することで、処理能力を向上させ、より多くのユーザーに提供することが可能になります。

## 技術課題
分散ネットワーク上でLLMを実行するにあたって、以下の技術課題を克服する必要があります。

- 分散処理: LLMを複数のノードに分割し、効率的に処理する必要があります。
- 通信: ノード間で効率的に通信を行う必要があります。
- 同期: ノード間でモデルの状態を同期させる必要があります。
- プライバシー: 訓練データやモデルパラメータを保護する必要があります。
<!-- - [超大規模言語モデルを分散ネットワーク上で動かす技術](#超大規模言語モデルを分散ネットワーク上で動かす技術) -->
## 組み合わせたら良さそうな技術
- Tokenの発行とDAO

## 超大規模言語モデルを分散ネットワーク上で動かす技術
基本的にP2Pを使う前提で書く。
現状で、GPUを（インターネット越しで）分散ネットワークとして使っている技術は見たことがない。
ただ、このプロジェクトではそれが必要になるはず。

### 手法1:単純に計算資源を束ねて計算する
いいところ：  
- 一つの大きな計算機を構成する方法のため単純でわかりやすい（簡単とは言ってない）
良くないところ：  
- 無駄が多いため、計算が遅そう
- 途中で一人でも抜けたら、計算ができなくなる
- P2Pネットワークを含めた最適化をしなければ、無駄に通信量が増える
- 計算が同期的でレイヤーが増えると計算時間が増える

### 手法2:モデルを多分割して、1部分を1人（1peer）が担う
レイヤーごとに分割してそれらを1peerが担う方法  
いいところ：  
- 安定感があり、1レイヤーを担うpeerが複数いれば可用性が上がる
良くないところ：  
- 計算が同期的でレイヤーが増えると計算時間が増える

### 手法3:小さなモデル（4B~13Bパラメータ）のモデルをそれぞれがホストし、多数決や直列、並列につなぎ合わせて性能を高める
いいところ: 
- とても簡単（apiのエンドポイントを互いに持っていればよいだけ、お互いに叩きあう）
- 今すぐ実装できそう（なんなら、LangChainとかで一人でもいける）
良くないところ: 
- 一定の成果は得られている([論文参照](https://arxiv.org/html/2402.05120v1))が、いわゆるScaling Lawからは外れる
